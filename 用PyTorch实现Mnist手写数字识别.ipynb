{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTz4YbR/cE8yuC6875LdE8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JcandZero/Test-task/blob/main/%E7%94%A8PyTorch%E5%AE%9E%E7%8E%B0Mnist%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#使用了基于Google colab中的谷歌计算资源，来进行计算"
      ],
      "metadata": {
        "id": "IUzrh5t6lGtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用了Pytorch架构进行mnist手写数字识别"
      ],
      "metadata": {
        "id": "W71RzLJumC78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#导入pytorch框架，导入其他包"
      ],
      "metadata": {
        "id": "XVKPvIFWmXG1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WqIBRAr7k5-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "定义超参数（paremeter）"
      ],
      "metadata": {
        "id": "ta5GUBlnmtfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 14#设置不同的epoch以测试\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "random_seed = 1\n",
        "torch.manual_seed(random_seed) #为了再现结果，设置random_seed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZEdTGtcm27l",
        "outputId": "85a670b1-2a8d-4861-e3bc-e7a5b00a82a2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f98a0e9df90>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.设置dataloader，包括train和test的data\n",
        "\n",
        "2.进行了特征预处理，Normalize，将数据集的全局平均值设置0.1307，标准偏差为0.3081\n"
      ],
      "metadata": {
        "id": "A_aSFMlbn5GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST('./data/', train=True, download=True,\n",
        "                               transform=torchvision.transforms.Compose([\n",
        "                                   torchvision.transforms.ToTensor(),\n",
        "                                   torchvision.transforms.Normalize(\n",
        "                                       (0.1307,), (0.3081,))\n",
        "                               ])),\n",
        "    batch_size=batch_size_train, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST('./data/', train=False, download=True,\n",
        "                               transform=torchvision.transforms.Compose([\n",
        "                                   torchvision.transforms.ToTensor(),\n",
        "                                   torchvision.transforms.Normalize(\n",
        "                                       (0.1307,), (0.3081,))\n",
        "                               ])),\n",
        "    batch_size=batch_size_test, shuffle=True)"
      ],
      "metadata": {
        "id": "aGTqGmkrnzL0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "查看数据"
      ],
      "metadata": {
        "id": "12SQevecp6bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "print(example_targets)\n",
        "print(example_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFQtau9ip58F",
        "outputId": "ae92bce8-9dd2-4c06-9483-cd1f3a101944"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3, 9, 4, 9, 9, 0, 8, 3, 1, 2, 3, 9, 1, 3, 6, 6, 4, 4, 9, 7, 3, 7, 6, 3,\n",
            "        4, 8, 4, 6, 8, 6, 1, 1, 1, 1, 0, 0, 1, 8, 4, 0, 1, 2, 7, 9, 3, 2, 3, 8,\n",
            "        3, 2, 0, 4, 6, 6, 5, 5, 3, 0, 3, 7, 2, 4, 1, 6, 7, 5, 4, 1, 0, 8, 5, 9,\n",
            "        0, 9, 6, 1, 8, 0, 9, 3, 5, 7, 8, 5, 6, 4, 2, 2, 2, 1, 8, 4, 4, 2, 1, 5,\n",
            "        9, 3, 7, 0, 4, 1, 7, 2, 2, 6, 5, 1, 2, 6, 1, 3, 1, 2, 7, 4, 1, 3, 2, 9,\n",
            "        3, 7, 2, 7, 4, 7, 0, 0, 9, 0, 9, 5, 1, 6, 9, 8, 1, 4, 6, 7, 5, 5, 6, 4,\n",
            "        5, 4, 0, 7, 4, 1, 3, 8, 2, 6, 2, 2, 8, 4, 1, 9, 6, 7, 5, 0, 7, 4, 6, 2,\n",
            "        6, 8, 7, 9, 8, 0, 7, 4, 2, 4, 9, 0, 4, 6, 5, 6, 6, 4, 4, 8, 3, 8, 0, 9,\n",
            "        5, 6, 3, 3, 8, 4, 4, 6, 7, 8, 5, 3, 4, 3, 3, 3, 3, 1, 8, 8, 9, 1, 0, 1,\n",
            "        4, 1, 0, 2, 0, 6, 5, 1, 5, 8, 8, 1, 3, 9, 4, 5, 9, 2, 5, 6, 5, 8, 2, 7,\n",
            "        4, 1, 6, 8, 7, 9, 3, 3, 4, 4, 5, 4, 9, 8, 2, 9, 1, 6, 7, 1, 0, 5, 1, 9,\n",
            "        2, 1, 5, 2, 7, 9, 0, 0, 6, 0, 8, 7, 2, 6, 1, 7, 6, 1, 2, 1, 6, 3, 1, 4,\n",
            "        1, 7, 5, 9, 3, 8, 6, 3, 8, 7, 4, 2, 3, 2, 4, 3, 5, 1, 2, 4, 0, 9, 4, 0,\n",
            "        5, 0, 7, 3, 3, 1, 8, 4, 1, 2, 2, 6, 0, 2, 1, 6, 3, 6, 2, 1, 7, 3, 9, 9,\n",
            "        0, 7, 6, 6, 8, 0, 1, 1, 9, 9, 6, 9, 4, 7, 8, 7, 8, 5, 3, 2, 4, 4, 3, 3,\n",
            "        1, 9, 5, 3, 1, 8, 4, 7, 6, 0, 5, 9, 7, 5, 4, 0, 3, 6, 0, 0, 5, 9, 7, 4,\n",
            "        7, 7, 3, 3, 8, 0, 5, 8, 6, 3, 4, 9, 6, 2, 2, 9, 3, 9, 4, 3, 7, 8, 0, 3,\n",
            "        7, 5, 1, 7, 0, 1, 7, 1, 6, 8, 9, 0, 2, 0, 0, 2, 8, 5, 6, 2, 1, 6, 4, 2,\n",
            "        1, 0, 7, 8, 7, 9, 8, 3, 1, 5, 2, 5, 0, 3, 8, 4, 1, 8, 0, 2, 4, 4, 0, 0,\n",
            "        4, 2, 8, 7, 5, 8, 5, 3, 7, 3, 3, 3, 1, 2, 4, 5, 9, 8, 2, 7, 6, 8, 6, 1,\n",
            "        3, 4, 0, 3, 7, 0, 3, 2, 9, 6, 2, 1, 7, 8, 1, 6, 9, 8, 6, 8, 7, 0, 5, 2,\n",
            "        5, 4, 4, 8, 6, 6, 8, 6, 7, 0, 9, 4, 4, 9, 7, 5, 7, 5, 9, 0, 8, 3, 9, 8,\n",
            "        1, 3, 3, 1, 0, 0, 1, 2, 1, 7, 0, 7, 0, 9, 8, 3, 2, 2, 0, 3, 0, 3, 0, 3,\n",
            "        6, 1, 5, 8, 8, 7, 4, 5, 2, 6, 7, 8, 4, 8, 3, 9, 2, 6, 8, 7, 2, 3, 0, 4,\n",
            "        9, 7, 7, 5, 0, 3, 6, 2, 9, 5, 9, 3, 7, 3, 6, 0, 9, 6, 5, 1, 6, 3, 9, 0,\n",
            "        9, 8, 4, 1, 6, 4, 1, 9, 1, 4, 6, 9, 4, 4, 7, 2, 3, 5, 9, 9, 2, 3, 0, 9,\n",
            "        9, 0, 0, 2, 4, 5, 6, 4, 7, 9, 0, 9, 0, 8, 3, 3, 1, 6, 0, 8, 3, 9, 8, 0,\n",
            "        3, 9, 7, 4, 8, 0, 9, 9, 7, 4, 9, 0, 1, 9, 0, 3, 3, 6, 8, 2, 6, 0, 0, 8,\n",
            "        8, 7, 9, 7, 7, 6, 1, 6, 0, 1, 9, 9, 5, 6, 9, 6, 2, 2, 1, 1, 6, 0, 7, 8,\n",
            "        4, 2, 4, 7, 0, 8, 9, 5, 9, 6, 9, 1, 2, 2, 8, 1, 3, 6, 2, 0, 5, 1, 1, 6,\n",
            "        8, 7, 0, 6, 2, 5, 6, 1, 9, 8, 6, 8, 5, 1, 7, 3, 2, 0, 9, 2, 2, 5, 5, 3,\n",
            "        8, 1, 4, 9, 7, 7, 1, 8, 2, 8, 1, 1, 7, 4, 1, 6, 9, 2, 0, 3, 4, 2, 0, 5,\n",
            "        7, 5, 7, 2, 5, 6, 8, 2, 6, 7, 3, 1, 6, 2, 4, 2, 0, 2, 5, 4, 1, 1, 5, 7,\n",
            "        4, 5, 2, 0, 8, 0, 4, 7, 3, 8, 8, 2, 3, 2, 6, 5, 1, 0, 4, 1, 3, 9, 0, 2,\n",
            "        8, 9, 1, 2, 0, 8, 7, 4, 4, 0, 7, 1, 0, 8, 5, 3, 3, 3, 4, 0, 7, 4, 4, 5,\n",
            "        0, 2, 2, 7, 8, 1, 2, 8, 0, 3, 5, 4, 6, 7, 9, 0, 0, 7, 3, 9, 4, 9, 5, 1,\n",
            "        4, 4, 9, 8, 7, 3, 9, 0, 8, 7, 0, 8, 4, 5, 8, 1, 7, 4, 2, 1, 1, 5, 1, 1,\n",
            "        5, 5, 6, 4, 3, 7, 3, 3, 3, 0, 4, 7, 9, 0, 3, 9, 8, 0, 7, 8, 7, 0, 0, 3,\n",
            "        7, 7, 6, 8, 8, 9, 8, 0, 9, 4, 9, 3, 8, 6, 8, 2, 0, 4, 8, 4, 4, 4, 6, 8,\n",
            "        3, 6, 3, 5, 1, 9, 5, 4, 3, 8, 3, 1, 2, 3, 1, 1, 9, 9, 5, 5, 0, 4, 1, 6,\n",
            "        9, 4, 8, 6, 8, 1, 7, 2, 6, 9, 8, 2, 7, 2, 2, 4, 8, 9, 3, 6, 2, 2, 7, 8,\n",
            "        5, 7, 2, 0, 0, 2, 3, 1, 1, 5, 8, 5, 3, 9, 7, 6])\n",
            "torch.Size([1000, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "利用matplotlib来绘制一些字符"
      ],
      "metadata": {
        "id": "z0eV2NaTqGdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "Y_aDObBWrP1w",
        "outputId": "e008dd72-1fe4-437a-a4b7-dca77bdd51ad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAELCAYAAAD+9XA2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAeh0lEQVR4nO3deZCVxbnH8V+LC6sgizuIyqJBcQMjEQlWrFKMXEUQNBYj5ZIYby4a0IghcUGJCSpuEZWbq+gNV0SiIhdBYrlvJIL7UpqrbKLCQEZFUFD6/nEOr92dOWfO0meZme+naqr6od/zvn1mmnnm7e7Tr7HWCgCAYm1X6QYAAJoGEgoAIAoSCgAgChIKACAKEgoAIAoSCgAgiiadUIwx3Y0x1hizfQWuvcwYc1y5r4s46DsoVHPuO0UnFGPM6caYxcaYL40xa9LlC4wxJkYDS8UYs8H52mqM2eTEZ+Z5rhnGmGsitu1YY8wbxpg6Y8w6Y8xDxpi9Yp2/WtB3StJ3jDFmojFmhTHmc2PMLGPMzrHOXy3oO/H7TnDuu9JJsUc+rysqoRhjxku6WdJ1knaXtJuk8yUdLWnHDK9pUcw1Y7HWtt32JWmFpKHOv83cdlwl/sqQ9Lak4621HSTtKel9SbdXoB0lQ98pmRpJo5X6Pu4pqZWkWyvQjpKh75SWMWagpP0LerG1tqAvSe0lfSlpeAPHzVDql+Gj6eOPk3SgpKck1Ul6S9K/Occ/JelcJx4j6Tkntkp1nvfTr79NkknXtZB0vaRaSR9I+vf08ds30MZlko5LlwdLWiXpUkmfSPrvsA1OO3pI+qmkLZI2S9ogaZ5zzoslvS7pM0n3S2pZwPd5J0nXSnq70J9VtX3Rd0rXdyTNkXSJE/9A0leSWlf6507fqe6+k3799pJekdR327Xy+fkUc4cyQKlfdnNzOPYnkiZLaidpsaR5khZJ2lXSf0iaaYzpnce1T5LUX6k3PVLS8el/Py9dd5ikfpJG5HFO1+6SOkraR6kfXEbW2umSZkqaYlN/ZQx1qkdKOkHSvum2jtlWkR7OGpjpvMaYbsaYOkmblOogUwp7K1WJvqPS9R1JJijvJKlnHu+hmtF3VNK+80tJz1hrXy/kDRSTUDpLqrXWfrPtH4wxL6QbvMkYM8g5dq619nlr7VZJh0pqK+n31trN1tonJP2vpDPyuPbvrbV11toVkp5Mn1NKfSNvstautNauV+ov+0JslXSFtfZra+2mAs8hSbdYa1en2zLPaaestR2stc9leqG1doVNDXl1lvQbSe8W0Y5qQ99pWKF9Z6Gkc9MTw+2V+otXkloX0ZZqQt9pWEF9xxjTVdLPJF1e6IWLSSjrJHV2x/qstT9I/xJcF5x7pVPeU9LK9A95m+WS8pl0/sQpb1SqoyTnDs5biLXW2q8KfK0rUztzlu4U90iaW8lx1cjoOw0rtO/cJek+pYZw3lLqF5+UGk5pCug7DSu079wkaZK19rNCL1xMQnlR0teSTs7hWHdL49WSuhpj3Gt3k/RRuvyl/L+mds+jTR9L6hqctxDhFsxem4wxYZtKvWXz9krdpjeV1Tr0nczHF8Vau9Vae4W1tru1dm+lkspH+u571NjRdzIfX6wfSbrOGPOJMWZbUnrRGPOTXE9QcEKx1tZJukrSNGPMCGNMO2PMdsaYQyW1yfLSxUplzV8ZY3YwxgyWNFTSrHT9q5JONca0Ti9ZOyePZs2WNNYYs7cxZhdJE/J8W5m8JqmPMeZQY0xLSVcG9Z9K2i/StWSMOdUY0zv9/ewiaaqkV9J3K40efccTu+90NMbsn14+/D2l+s6k4C/zRou+44nadyT1knSIUkNk24bJhkp6KNcTFLVs2Fo7RdI4Sb9S6s19KulOpcZtX8jwms3pRg5RalXENEk11tptcwQ3KrVy4VOlhnpm1neeDP5T0mNK/SCWSnowv3dUP2vte5ImSXpcqVUe4Rjkf0n6Xnoc9+Fczpled35Mhuq9lBoL/0LSG0qNrQ4rpO3Vir6TiN13Ouu7lU0LJN2VnsBtMug7iah9x1q7xlr7ybav9D/X5jOfs23ZGwAARWnSW68AAMqHhAIAiIKEAgCIgoQCAIiChAIAiCKvT14bY1gSVoWstdW+ZTf9pjrVWmu7VLoR2dB3qla9fYc7FKD5KnSLEKDevkNCAQBEQUIBAERBQgEAREFCAQBEQUIBAERBQgEAREFCAQBEQUIBAERBQgEAREFCAQBEQUIBAERBQgEAREFCAQBEQUIBAERBQgEARJHXA7aqQa9evZLy4Ycf7tVt2LDBi3v27JnxPH379vXimpqanNuw3Xbf5eGtW7dmPG7UqFFePGfOnJyvgerwwgsvePGAAQO8eNy4cUn5xhtvLEubgGrFHQoAIAoSCgAgChIKACAKY63N/WBjcj84EnfORJLmz5+flPfaay+v7ttvv/XiVq1aJWVjjFeXz/sOuefKdp6ZM2d68VlnnVXwNbOx1pqGj6qcSvSbYrjzJuGcSTbdunXz4pUrV0ZrU4kssdb2q3QjsmlsfSeWI444wosfffTRpLxo0SKvbvTo0WVpU6DevsMdCgAgChIKACCKql82HC4N3rJlS1LecccdCz7vunXrvNg9V7t27Qo+75tvvpmUZ8yYUfB5UD433HCDF7vDXOGw1dFHH+3FK1asyHiekSNHxmoimpnzzjvPizt16pSUDzjggHI3J2fcoQAAoiChAACiIKEAAKKo+jmUWbNmefFzzz2XlI866qiCzxvOoUyZMiUpH3bYYTmf58UXX/TiU045JeM1UJ1OO+20jHXhUuCQ+/MPz9O1a1cvbgTLiFGPffbZJynfe++9Xt1NN93kxQ899FCUa3bp0sWL3Y8qhB+BqCbcoQAAoiChAACiIKEAAKKo+jmU0KpVq5JyPtvB//CHP/Ti8ePHe3E+8yZPP/10Ur7uuuu8OuZNGp9wrmPq1Kk5v9btj6ERI0Z4MdvbN05ufwg/h9S6dWsvjjWH4s7FSv4WT8VsG1Vq3KEAAKIgoQAAomh0Q175cIe5nnrqKa8u25MWv/jiCy/+05/+5MUXX3xx8Y1Dxfzyl7/MWh8uBc0m25LjcKdihrwah4kTJ3rxqaeempTD3xu1tbUlaYP7VNjwuiwbBgA0eSQUAEAUJBQAQBRNag5lyJAhXnzfffcl5XDsM9vSO/dJj1Jx2+Sj+jT0FMZsW6Q0NP/i2nvvvXM+FpUTzplMmDDBi93fHeHvjcmTJ5ekTdl+X/3lL38pyTVj4A4FABAFCQUAEAUJBQAQRZOaQxkzZowXt23btqDztGjRwosvuOACLx47dmxB50V1KGZuI9vnTkINzdWgctzt4c8880yvLtxOZePGjUm5pqbGq3MfpxFTts+aVPP2TtyhAACiIKEAAKJoUkNed999txcfeeSRSfnZZ5/16h555BEvdncf7t+/f9brdOjQISnX1dXl3U5UVviUzXBoyn0S6EsvvZT1WNcDDzzgxfkMj6G8LrvssqTcu3dvry5cGvzuu+8m5Vi7CTckbEM17zDs4g4FABAFCQUAEAUJBQAQRZOaQ1m4cKEX77vvvjm/1n3yXkNLAX/zm98kZbayb3zC7enHjRvnxbNnz07KDf183af5hefJ58mPKK9jjjkmKWfbKl6SRo8eXfL2DBo0yIureYv6bLhDAQBEQUIBAERBQgEARNGk5lCK4X7eINweesSIEV7cr1+/pBxu77Jhw4YStA4xhdvTZ/v8yP3335/1XF27ds35uiNHjkzKYZ9y61B677zzTlI+/PDDvbrwMx/uo5tL9TmUgQMHZm0Dn0MBADQrJBQAQBQmn1spY0zjuO8qUjjMMXz4cC92l/R169bNq/voo49K17AMrLVVvcawsfUb96mMMZf+ulu+jBo1yqvL9pTIElpire3X8GGVU46+8/LLL3txuBVLmzZtknL4+zJc3uvWZ6sL6/M5bzhcWq7tYAL19h3uUAAAUZBQAABRkFAAAFGwbLgAq1evTsqbN2+uYEtQCu4y0XzmUMLlx+4jEaSKzZOgAe7HACR/a3tJuuaaa5JyQ3PObn34yIzQgQcemPG87hMlc7luteAOBQAQBQkFABAFCQUAEEVF5lB22203L/7iiy+S8saNG8vdnLy5n1NZu3ZtBVuCUshnOxUX26c0Dddee23WuBSGDRvmxeH2T65wfqWacIcCAIiChAIAiKJsQ16XX355Uj7vvPO8uieeeCIpn3XWWeVqUsHmzZtX6SaghC666KKcj3W3UwEKFW6fwm7DAIBmjYQCAIiChAIAiKJkcyjuFuCSdMUVV2Q89qSTTkrK4dPTli5dGrdhGUyYMCEph9tDh55++ulSNwcVNGDAgJyPHTduXAlbguYq3L6+seAOBQAQBQkFABBFyYa83nvvPS92PwHfqlUrr659+/ZJ+a9//atXd/7553uxu2PrSy+9lHN7evXq5cWjR4/2Yndn2MayRA9xhJ+MzzbkFe4onE8fBHLFsmEAQLNGQgEAREFCAQBEUbI5lPnz53vx2LFjk/KFF17o1R100EFJ2Z1PkaRZs2Z5cW1tbVL+xz/+kXN79thjDy/u1q1bxmPD+Z8ZM2bkfB00PvlstcKOwiiHdevWeXGnTp2ScjUvKeYOBQAQBQkFABAFCQUAEEXZtq+/++67k/Ijjzzi1R177LEZX3fnnXd6sTuW6JYbEo47huu616xZk5Tdz6RI0oIFC3K+DpqW8HMnQDk8+OCDXnzuuecm5Wr+TAp3KACAKEgoAIAoyjbk5QqXxM2ZMyfjsR988IEXDxo0KCmHS3/dpckNefXVV7146NChSfnjjz/O+Txo/BYvXpyxburUqWVsCZCydu1aL3aH7KdPn17u5uSMOxQAQBQkFABAFCQUAEAUJp8laMaY6l2v1oxZa6t3LwbRb6rYEmttv0o3Ipvm2nf22WcfL77nnnuS8uDBg8vcmnrV23e4QwEAREFCAQBEQUIBAETBHEoTwBwKCsQcCgrFHAoAoHRIKACAKEgoAIAoSCgAgChIKACAKEgoAIAo8t2+vlbS8lI0BAXbp+FDKo5+U53oOyhUvX0nr8+hAACQCUNeAIAoSCgAgChIKACAKEgoAIAoSCgAgChIKACAKEgoAIAoSCgAgChIKACAKEgoAIAoSCgAgChIKACAKEgoAIAomnRCMcZ0N8ZYY0y+2/THuPYyY8xx5b4u4qDvoFDNue8UnVCMMacbYxYbY740xqxJly8wxpgYDSwVY8wG52urMWaTE5+Z57lmGGOuidg2Y4yZaIxZYYz53Bgzyxizc6zzVwv6Dn2nUPSd+H0nfc6fGGOWp7+vDxtjOubz+qISijFmvKSbJV0naXdJu0k6X9LRknbM8JoWxVwzFmtt221fklZIGur828xtx1XirwxJNZJGK/V93FNSK0m3VqAdJUPfKRn6Tv2voe80wBjTR9KdSvWf3SRtlDQtr5NYawv6ktRe0peShjdw3AxJt0t6NH38cZIOlPSUpDpJb0n6N+f4pySd68RjJD3nxFapzvN++vW36bsHhbWQdL1ST3n7QNK/p4/fvoE2LpN0XLo8WNIqSZdK+kTSf4dtcNrRQ9JPJW2RtFnSBknznHNeLOl1SZ9Jul9Syxy/t3MkXeLEP5D0laTWhf68qumLvkPfoe9UZd/5naT/ceL90+dvl+vPp5g7lAGSdpI0N4djfyJpsqR2khZLmidpkaRdJf2HpJnGmN55XPskSf0l9ZU0UtLx6X8/L113mKR+kkbkcU7X7pI6KvWYy59mO9BaO13STElTbOqvjKFO9UhJJ0jaN93WMdsqjDF1xpiBWU5tgvJOknrm8R6qGX1H9J0C0XdUsr7TR9JrzjX+T6mE0ivXN1BMQuksqdZa+822fzDGvJBu8CZjzCDn2LnW2uettVslHSqpraTfW2s3W2ufkPS/ks7I49q/t9bWWWtXSHoyfU4p9Y28yVq70lq7XtK1Bb63rZKusNZ+ba3dVOA5JOkWa+3qdFvmOe2UtbaDtfa5DK9bKOnc9ORee6X+apGk1kW0pZrQdxpG36kffadhhfadtkrd1bg+Uyoh56SYhLJOUmd3rM9a+wNrbYd0nXvulU55T0kr0z/kbZZL2iuPa3/ilDcq9Y1Izh2ctxBrrbVfFfhaV6Z2NuQuSfcpdRv+llKdV0rdEjcF9J2G0XfqR99pWKF9Z4OkcAHHzpK+yPXCxSSUFyV9LenkHI61Tnm1pK7GGPfa3SR9lC5/Kf+vqd3zaNPHkroG5y2EDWKvTcaYsE3h8UWx1m611l5hre1urd1bqV8MH+m771FjR9/JfHxR6Dse+k5+3pJ0iHO9/ZQaXnwv1xMUnFCstXWSrpI0zRgzwhjTzhiznTHmUEltsrx0sVJZ81fGmB2MMYMlDZU0K13/qqRTjTGtjTE9JJ2TR7NmSxprjNnbGLOLpAl5vq1MXpPUxxhzqDGmpaQrg/pPJe0X6VoyxnQ0xuyfXgL6PUlTJU0K/rpqtOg7HvpOHug7nqh9R6k5maHGmGOMMW0kTZL0oLW2LHcostZOkTRO0q+UenOfKrXs7FJJL2R4zWalfpBDlFoVMU1SjbX23fQhNyo1EfSppHuUepO5+k9Jjyn1g1gq6cH83lH9rLXvKfXNfVypVR7hGOR/Sfpeehz34VzOmV53fkyG6s76bnXKAkl3pSfhmgz6ToK+kyf6TiJq37HWvqXUSraZktYoNXdyQT5t3rbsDQCAojTprVcAAOVDQgEAREFCAQBEQUIBAERBQgEARJHXjpbGGJaEVSFrbbVv2U2/qU611toulW5ENvSdqlVv3+EOBWi+Ct0iBKi375BQAABRkFAAAFGQUAAAUZBQAABRkFAAAFGQUAAAUZBQAABRkFAAAFHk9Ul5AEAc22333d/zRx55ZNZj33///aS8bt26krWpWNyhAACiIKEAAKIgoQAAomAOBQAicedFjPE3AbfW3zj56quvTsqXXXZZ1vOuXLkyKf/sZz/z6hYuXJh3O0uFOxQAQBQkFABAFCa8Dct6MA+7qUo8YAsFWmKt7VfpRmRTbX2nc+fOXnzzzTd78c4775yUFy9e7NVNmzbNi+fOnZuU3SEtSVq+3H/cyNlnn52U27Vr59VNnDgxKd94440Z2x5ZvX2HOxQAQBQkFABAFCQUAEAUzXbZcPfu3b148ODBSfmII47w6s444wwvdpcDnnjiiV5dOG4KoHFz50VuueUWr+7000/34vXr1yflZcuWZayTpB//+MdJ+fPPP8/ahjlz5iTlcJ5k0qRJSXnr1q1eXTjHU2rcoQAAoiChAACiaNLLho8//vik7N5eStKZZ57pxe3bt8/5vO6Q15o1a7y6Aw88MCnX1dXlfM5isGw4P+4QhiSNHj3aiy+99NKk3LVrV68u2/+X8Of9hz/8wYvvvvvupBz2mwph2XA9DjroIC9etGhRUu7QoYNXd8kll3jx9OnTk/KWLVtK0Dqpd+/eXnzttdcm5aOOOsqrO/jgg7044k7FLBsGAJQOCQUAEAUJBQAQRaObQ3HHMC+66CKv7vzzz/fiXXbZJSlvv72/Qjp83xs3bkzK4dhnOL/izqGE59ljjz2S8tq1a//1DZQAcygNc5+IN3v2bK+uW7duGV+3atUqL872/2XPPff04hYtWnjxAw88kJRHjRqVubHlwxyKpP3228+Ln376aS/ea6+9kvKf//xnr66mpqZ0DcuRO1c8f/58r27JkiVePHDgwKRc5BwPcygAgNIhoQAAoiChAACiqPqtV3r16uXFs2bNSsp9+/bN+TzhuOjDDz/sxY8//nhS3rx5s1f34osvenHHjh0zXscdRy/XHAoaduWVVyblcM7krbfe8uIbbrghKYdj5t98803Ga4wfP96LJ0yY4MVuX27VqpVXt2nTpoznRWmFc6/unIkkffzxx0l57NixZWlTPh577LGkvHTpUq+uf//+XnzSSScl5Yceeih6W7hDAQBEQUIBAERRdcuGw519b731Vi8Otz5wrV69OuO5nn/++Zzb0KZNGy9+5ZVXvHj//fdPyuH3r7a2Nilfd911Xp07lBITy4b/1fDhw734/vvvT8rh0/DcJcVSvO0p/va3v3lxv37frbIMh8OmTJkS5Zp5arbLht3vv7tbryR9/fXXXuwOG7377rulaE40BxxwgBe//fbbXuwO74b9Ps9hV5YNAwBKh4QCAIiChAIAiKIiy4Z33XVXL3a3gB43bpxX525zIkmfffZZUv71r3/t1d1+++1R2hcutXPnTOprk6tLly5JOdwyv1RzKPhX4XLz7bb77m8nd55Lirqld87CcXqUVp8+fbx44sSJSTnclunyyy/34mqfN3GF84PhNkMjR45MyuHW9uGcXyG4QwEAREFCAQBEQUIBAERRtjmUwYMHJ+XbbrvNq3MfaRl+riP8DIi7ftzdLqWY9kj+3Ic7zlhfm2LUoXLCrXPCxxO483T5mDx5shf37NnTi19++eWkfMcddxR0DRTmwgsv9GL3s2bhtkzho5sbk/CzJPfcc48Xn3baaSW9PncoAIAoSCgAgChKNuQVLsW7+uqrk7I7xNWQU0891Yu/+uqrpBwuPw6fmHfyyScn5QsuuMCr23nnnb14hx12yLlNuXKfAonyCodV3afaDRo0yKtzd5OVpGXLliXlJ598Mut1jj322KQc9utwebm7+zTLhssr/D3i7hodDnE1paHqBQsWeLH7+3PYsGFeHcuGAQBVg4QCAIiChAIAiKJkcyjhePKAAQMKOs+HH37oxYWOb4bj2aUaJ33nnXeS8s9//vOSXAMN+/zzz734oosuSsrhFj3f//73vdjdAjzcDrwY4ZMhUT6dOnXyYnduYeHCheVuTsW4vwfD70kM3KEAAKIgoQAAoiChAACiKNkcyvr1673YfTxv+HmRcpg2bZoXz50714vd9o0ZM8arC7fUz+aBBx5IyqtWrcqjhSilV199NSkPGTLEqzvmmGMyvq5Hjx5evN9++3mx289/+9vfZm2Du/UKyuuNN97w4rZt2yblcJ6sMW1Xn6/XX389Kfft2zf6+blDAQBEQUIBAERRsiGvcDuLE088MSmHT0Rs2bJlxvM8++yzXvzmm29GaN2/ateuXVI+5ZRTvLpwybH79L+6ujqvzt1eA9Up/JnNmzev4HPNmjUrY93WrVuzXhfls3jxYi8+55xzkvLQoUO9uqY85OVur/KLX/wi+vm5QwEAREFCAQBEQUIBAERRtic2unMfpZoHKcbpp5+elMOloeE2Le7Y+B//+EevLtzWA01btsceLF261IsXLVpU6uYgg3Cuy51DmTRpklc3e/ZsL16+fHnpGlZiNTU1Xjx69OiknG3+r1DcoQAAoiChAACiIKEAAKIo2xxKtenTp48XT548uaDzPP744zGagyYo/OwDKiecz3rmmWeScvhI6DvuuMOLzz777KQcfr6u2rhzwZI0depUL27fvn1SLsXjFLhDAQBEQUIBAETRbIe83Cf4SVLHjh1zfq27fcHbb78drU2ofuE2QQcddFCFWoJ8/POf//TiYcOGJWV3B15JOuGEE7zYXe591VVXeXWbNm3yYndX4xUrVhTW2HrsvvvuSblz585e3cSJE5PyyJEjvTp3myjJ325l+vTp0dqXXC/6GQEAzRIJBQAQBQkFABBFs5lDCbekHzVqVMHnuvfee5My29U3LzvttJMX9+zZM+Ox8+fPL3VzUCB3TiV8cmH4NNeBAwcm5XBbltCXX36ZlP/+978X00SP+zGHXXfd1atzt4Z67bXXvLopU6Z4sdv+b7/9Nlr7tuEOBQAQBQkFABBFsxnyCncUbdOmTcHnCp8iieZj8ODBOR9bW1tbuoYgmnBJ8Y9+9CMvHjJkSFIePny4V9eqVSsv7t27d1Lu1KmTV3fwwQfn3KbwaZ/up9o//PBDr+53v/tdUl6wYIFXt2XLlpyvGQN3KACAKEgoAIAoSCgAgCia9BxKjx49knK4u3D4FMZsbrjhBi+uxidOojy6d+9e6SagxMJ5h0ceeaTecr6OPPLInI8Nl/QuWbKk4OuWE3coAIAoSCgAgChIKACAKJr0HMp9991X0OuWL1/uxddff32M5qAJeO+993I+1v1MgiS9/PLLsZuDRsR97EVTxR0KACAKEgoAIIomPeS1fv36nI91lwpec801Xh07CmObZ555xovXrFmTlMNdYI866igvnjlzZukaBlQB7lAAAFGQUAAAUZBQAABRmHy2IDHG5H5wFejYsWNSfvLJJ726li1berG7vX1jG+u21ppKtyGbxtZv8jFjxoykXFNT49XV1dV58YknnpiUX3rppZK2K0dLrLX9Kt2IbJpy32nk6u073KEAAKIgoQAAoiChAACiaDafQznkkEMq2BI0VePHj0/K4SMSWrdu7cVdunQpS5uASuEOBQAQBQkFABBFkx7yAkpt3bp1Sbl///4VbAlQedyhAACiIKEAAKIgoQAAosh3DqVW0vIGj0I57VPpBuSAflOd6DsoVL19J6+9vAAAyIQhLwBAFCQUAEAUJBQAQBQkFABAFCQUAEAUJBQAQBQkFABAFCQUAEAUJBQAQBT/Dxde9SJUdQrYAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#神经网络布置-使用CNN网络（提供两种sequential函数的方法）\n",
        "\n",
        "其中神经元的单元选用Conv2d的2d卷积层，和两个线性函数"
      ],
      "metadata": {
        "id": "fTh_wO_JrWGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "SWNCPZoNrjWp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**更换神经网络层**（此层代码与上面的代码选择使用）"
      ],
      "metadata": {
        "id": "LkFPlLsFVI_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "此层函数使用的较为复杂，混合使用Conv2d、BatchNorm2d、Relu等函数\n",
        "\n",
        "已全部注释化"
      ],
      "metadata": {
        "id": "MfZh-QxiWY89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "          \n",
        "#         self.features = nn.Sequential(\n",
        "#             nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(32),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "#             nn.BatchNorm2d(32),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(64),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm2d(64),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         )\n",
        "\n",
        "# def forward(self, x):\n",
        "#         x = self.features(x)\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         x = self.classifier(x)\n",
        "        \n",
        "#         return F.log_softmax(x) "
      ],
      "metadata": {
        "id": "HvDsTxsAUuzc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 广义地说，我们可以想到torch.nn层中包含可训练的参数，而torch.nn.functional就是纯粹的功能性。forward()传递定义了使用给定的层和函数计算输出的方式。为了便于调试，在前向传递中打印出张量是完全可以的。在试验更复杂的模型时，这就派上用场了。请注意，前向传递可以使用成员变量甚至数据本身来确定执行路径——它还可以使用多个参数!\n",
        " "
      ],
      "metadata": {
        "id": "uNvn28SVENWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在让我们初始化网络和优化器。\n"
      ],
      "metadata": {
        "id": "LMk6fP8-sCN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = Net()\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
        "                      momentum=momentum)"
      ],
      "metadata": {
        "id": "x8Oj1LmXsAn2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#模型训练"
      ],
      "metadata": {
        "id": "prQ6dNxPsIzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CEAzgxxgsO28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
      ],
      "metadata": {
        "id": "g0kzAk7csH_m"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "先使用随机初始化的网络参数来计算损失，看看准确度"
      ],
      "metadata": {
        "id": "x6F7o7MEs2Qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = network(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      torch.save(network.state_dict(), './model.pth')\n",
        "      torch.save(optimizer.state_dict(), './optimizer.pth')\n",
        "          \n",
        "train(1) #训练一个epoch"
      ],
      "metadata": {
        "id": "vgXXWkAvsyaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d0e588-1273-44a5-cdc5-e8b82c2b2ab1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-f14b5dfe1d84>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.319280\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.290954\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.318535\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.261000\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.259137\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.229576\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.196151\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.181698\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.088444\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.014765\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.818533\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.822772\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.838733\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.530556\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.738141\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.420971\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.369238\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.335397\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.182360\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.056649\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.085395\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.043788\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.979933\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.015183\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.937397\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.883366\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.790303\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.978384\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.981849\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.961201\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.976320\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.951693\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.700487\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.847359\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.646499\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.727109\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.849435\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.613159\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.982897\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.527990\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.722221\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.714018\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.817023\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.826285\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.581746\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.619269\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.816162\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.415688\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.516556\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.546946\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.556581\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.585459\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.687491\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.625588\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.433444\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.731643\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.520729\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.431021\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.661085\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.590926\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.600679\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.521783\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.477349\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.633068\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.552758\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.822104\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.538440\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.561818\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.677583\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.570780\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.460455\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.458421\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.649541\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.447023\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.496603\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.515545\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.385327\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.624006\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.449085\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.374284\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.647305\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.406169\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.341257\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.382736\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.574533\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.408767\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.439887\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.374103\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.743120\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.465267\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.502371\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.306819\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.695309\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.380143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = network(data)\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC8J6UkEs8l1",
        "outputId": "1318329f-39e1-4f1d-f01e-0cf72e1418df"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-f14b5dfe1d84>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 0.1911, Accuracy: 9439/10000 (94%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以避免将生成网络输出的计算结果存储在计算图中。是时候开始训练了!我们将在循环遍历n_epochs之前手动添加test()调用，以使用随机初始化的参数来评估我们的模型。"
      ],
      "metadata": {
        "id": "8O0bE_8ltFgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test()# 不加这个，后面画图就会报错：x and y must be the same size\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfJmbs0ctElO",
        "outputId": "1f33b4c1-8d55-4513-cb92-5879cedf38e0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-f14b5dfe1d84>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 0.1911, Accuracy: 9439/10000 (94%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.359436\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.382616\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.622312\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.488098\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.297355\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.182658\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.457787\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.492744\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.467196\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.702105\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.481813\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.351125\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.397469\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.561537\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.464565\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.446449\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.304118\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.577272\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.347102\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.376137\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.540452\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.400099\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.535876\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.355860\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.235569\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.221576\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.386148\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.370337\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.314516\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.314762\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.596766\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.384605\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.499322\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.415698\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.557756\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.557688\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.471818\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.311361\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.476906\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.241712\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.165668\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.399019\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.399043\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.213818\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.222878\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.290120\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.516315\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.287536\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.293928\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.497053\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.630162\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.434521\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.340859\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.406907\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.453515\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.437435\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.481814\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.364647\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.461105\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.379560\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.304479\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.286621\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.377156\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.180908\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.476517\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.155784\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.333759\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.493314\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.267939\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.428113\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.297536\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.466330\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.282059\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.329461\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.225850\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.316815\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.343405\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.514568\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.298121\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.322827\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.428666\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.334111\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.433545\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.277645\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.218585\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.198461\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.521711\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.244698\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.413639\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.251667\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.521914\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.464894\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.338672\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.237215\n",
            "\n",
            "Test set: Avg. loss: 0.1229, Accuracy: 9629/10000 (96%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.256502\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.190123\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.361356\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.199604\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.205620\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.329155\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.240677\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.410590\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.594199\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.329787\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.412796\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.297131\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.260667\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.171792\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.284931\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.273357\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.283531\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.231010\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.368930\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.210425\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.513579\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.431587\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.374809\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.381088\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.290591\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.824302\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.446375\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.437145\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.355924\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.258295\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.358305\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.409451\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.199548\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.227982\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.284493\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.264547\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.398188\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.183788\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.282646\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.164197\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.450678\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.230096\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.170000\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.168552\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.344211\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.644646\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.163667\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.220350\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.182677\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.276293\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.423058\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.402297\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.185191\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.343828\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.308342\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.293518\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.622865\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.244155\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.237978\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.364266\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.304705\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.288722\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.250939\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.232745\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.420262\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.346601\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.391353\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.216439\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.306773\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.337783\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.157392\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.225085\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.341044\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.180299\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.479111\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.445796\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.184423\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.256878\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.417864\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.372240\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.428440\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.243491\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.447469\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.196941\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.154418\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.466945\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.302824\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.181657\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.261293\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.345623\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.620282\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.384469\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.124993\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.341718\n",
            "\n",
            "Test set: Avg. loss: 0.0986, Accuracy: 9678/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.218881\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.165984\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.312811\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.324011\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.178362\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.145238\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.193637\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.315645\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.255748\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.289269\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.191543\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.146686\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.337138\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.116123\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.148367\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.361809\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.532553\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.206347\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.215183\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.400101\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.135044\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.302469\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.491485\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.293571\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.278238\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.079051\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.187221\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.306986\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.188835\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.262164\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.236932\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.163704\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.248176\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.256485\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.282426\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.135926\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.247788\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.125058\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.175477\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.234079\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.196425\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.131201\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.136945\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.237155\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.426020\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.182006\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.259393\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.151175\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.224495\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.140606\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.246941\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.210468\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.453553\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.183865\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.196840\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.284886\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.389386\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.217670\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.112103\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.288729\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.185688\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.312716\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.163582\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.170345\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.362697\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.196100\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.177883\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.414063\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.379831\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.166720\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.163034\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.134310\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.308222\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.331843\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.378267\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.415628\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.294394\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.166918\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.294664\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.159037\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.369257\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.314946\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.179534\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.264617\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.360094\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.443241\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.175973\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.322657\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.128070\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.491101\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.337251\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.401136\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.135563\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.339653\n",
            "\n",
            "Test set: Avg. loss: 0.0862, Accuracy: 9718/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.243778\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.342769\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.215689\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.280162\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.229276\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.160675\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.137629\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.303867\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.376828\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.173678\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.152195\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.116965\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.300197\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.236321\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.420905\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.401455\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.340904\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.232437\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.240460\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.083321\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.244266\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.401494\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.227215\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.209912\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.330909\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.464422\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.251526\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.346759\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.102285\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.135839\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.119786\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.606417\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.465088\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.175132\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.118322\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.250174\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.258249\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.199965\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.156262\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.275860\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.281517\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.185288\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.181561\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.232328\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.369495\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.569984\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.167612\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.113384\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.328761\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.302255\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.181605\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.205551\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.537536\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.184210\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.156243\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.226133\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.215337\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.222131\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.295251\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.385569\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.260622\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.415320\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.092602\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.292410\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.306093\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.157167\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.215457\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.295079\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.177610\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.421414\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.223419\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.147750\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.212072\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.199057\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.105950\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.272902\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.238137\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.325157\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.178653\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.195152\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.150112\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.268988\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.261576\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.292179\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.235027\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.162339\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.103594\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.292574\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.212490\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.251926\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.281083\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.519383\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.283925\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.240769\n",
            "\n",
            "Test set: Avg. loss: 0.0746, Accuracy: 9775/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.367417\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.352236\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.300442\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.153866\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.344181\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.256411\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.178007\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.267300\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.135322\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.213228\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.170476\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.320806\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.229842\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.144441\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.406373\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.218493\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.308627\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.207050\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.210825\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.164329\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.193579\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.129250\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.172534\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.175323\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.194300\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.210148\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.289897\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.155980\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.145911\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.187036\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.263016\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.160157\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.278308\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.115073\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.066798\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.502865\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.309574\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.156335\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.177867\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.448653\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.135703\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.250954\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.301742\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.359112\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.063537\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.138768\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.107806\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.294862\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.303098\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.147837\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.148573\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.248637\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.093100\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.194939\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.328837\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.157707\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.210021\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.274428\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.207789\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.165695\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.155978\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.062619\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.272772\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.235528\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.091824\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.232944\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.417844\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.146068\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.129135\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.350032\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.295181\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.146745\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.259536\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.201177\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.179028\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.310833\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.266191\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.171070\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.273866\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.204507\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.196079\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.386226\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.117310\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.162830\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.272046\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.175954\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.246980\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.172608\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.165263\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.265056\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.344356\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.173474\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.112728\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.315129\n",
            "\n",
            "Test set: Avg. loss: 0.0672, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.134076\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.117222\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.177692\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.195753\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.180281\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.252479\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.105601\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.121805\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.227717\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.191311\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.231725\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.141403\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.148114\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.451766\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.160294\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.095862\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.175196\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.202322\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.266866\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.195933\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.170819\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.257253\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.184479\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.296271\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.197125\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.185588\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.203478\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.476786\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.194979\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.114805\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.155836\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.161969\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.097001\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.188131\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.163685\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.165418\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.335648\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.246352\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.121687\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.500977\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.174474\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.242249\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.233444\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.167126\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.291541\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.282525\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.142320\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.222170\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.325821\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.262082\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.186301\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.214019\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.210991\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.212641\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.078238\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.352687\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.093244\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.235969\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.150425\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.254659\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.240981\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.116985\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.224388\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.122979\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.158651\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.152374\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.276037\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.234376\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.254454\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.062307\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.210180\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.164665\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.309175\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.214374\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.194618\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.167592\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.084410\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.082709\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.184839\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.206722\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.279312\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.281907\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.255191\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.132867\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.134142\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.294263\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.156852\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.126394\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.336226\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.238918\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.231784\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.231289\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.085810\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.247195\n",
            "\n",
            "Test set: Avg. loss: 0.0638, Accuracy: 9798/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.283044\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.096457\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.253154\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.135509\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.233398\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.188814\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.169184\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.201257\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.242582\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.123798\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.117722\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.227523\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.098270\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.264403\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.191025\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.080992\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.141259\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.086406\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.119722\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.147801\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.148792\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.221852\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.331526\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.190143\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.451957\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.186637\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.184420\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.134274\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.134329\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.299155\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.225515\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.181300\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.287244\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.100267\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.178146\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.192826\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.067279\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.074584\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.123820\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.365061\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.166596\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.236512\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.109532\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.111797\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.068682\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.184213\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.376506\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.399765\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.164487\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.273236\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.132992\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.197803\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.243838\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.172255\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.181715\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.083979\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.153147\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.264996\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.235841\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.216694\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.153456\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.153183\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.139268\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.251850\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.180859\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.165279\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.237331\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.208667\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.236715\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.218062\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.376328\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.177881\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.207505\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.088437\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.168209\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.213269\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.113360\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.219612\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.142477\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.114491\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.119629\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.241111\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.343350\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.391151\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.150827\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.088171\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.037358\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.233125\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.248720\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.215184\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.121560\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.194105\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.202706\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.092012\n",
            "\n",
            "Test set: Avg. loss: 0.0600, Accuracy: 9802/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.268184\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.021493\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.359516\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.162799\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.086678\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.154837\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.214339\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.156550\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.094504\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.140066\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.229106\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.134020\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.100070\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.060535\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.331619\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.159054\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.088879\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.483366\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.111413\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.112544\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.304373\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.044925\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.160963\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.195292\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.259851\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.125892\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.193414\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.174774\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.325667\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.095509\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.329360\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.365386\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.183709\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.141595\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.301574\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.154607\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.102166\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.097970\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.252239\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.247931\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.173794\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.129884\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.130428\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.143793\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.312013\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.197583\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.128449\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.107446\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.102253\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.231730\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.217749\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.250759\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.159178\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.125218\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.052189\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.195625\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.066096\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.063769\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.200697\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.167866\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.099288\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.055136\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.091325\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.088619\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.102162\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.151905\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.126067\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.164635\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.332424\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.245586\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.168321\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.087699\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.181540\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.166022\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.069842\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.184659\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.227479\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.133636\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.270497\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.183340\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.128349\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.234936\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.063211\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.078994\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.147991\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.214302\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.141877\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.123683\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.210182\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.169652\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.262968\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.196013\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.143546\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.203997\n",
            "\n",
            "Test set: Avg. loss: 0.0541, Accuracy: 9816/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.203267\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.133609\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.163848\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.271144\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.199639\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.315839\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.117468\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.137361\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.127278\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.163905\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.129706\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.117189\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.189189\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.195274\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.242208\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.132873\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.118992\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.194844\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.082873\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.187659\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.221130\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.146888\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.117950\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.111665\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.224929\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.093516\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.315541\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.156784\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.256208\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.091964\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.100441\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.096080\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.243923\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.129766\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.295863\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.181538\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.144534\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.184077\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.258692\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.147537\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.092372\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.105972\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.193190\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.047422\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.075421\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.236134\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.191409\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.162503\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.060619\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.094907\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.161729\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.079631\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.180843\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.287817\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.279648\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.213976\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.198674\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.222106\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.195957\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.178321\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.120376\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.242218\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.123556\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.097753\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.262483\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.139942\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.085590\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.208294\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.181590\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.392408\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.426803\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.071216\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.316204\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.140217\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.283203\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.185922\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.187325\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.069976\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.035497\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.303728\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.180917\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.382522\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.096489\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.079057\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.064192\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.098269\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.153296\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.119169\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.339517\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.196745\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.108031\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.056372\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.362502\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.263905\n",
            "\n",
            "Test set: Avg. loss: 0.0536, Accuracy: 9825/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.189302\n",
            "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.146657\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.280953\n",
            "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.216541\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.219947\n",
            "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.431017\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.231067\n",
            "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.157685\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.215050\n",
            "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.217314\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.232961\n",
            "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.362130\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.238901\n",
            "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.057115\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.129177\n",
            "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.248226\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.075462\n",
            "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.121596\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.043568\n",
            "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.226618\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.079754\n",
            "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.123856\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.192304\n",
            "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.065210\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.320016\n",
            "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.082675\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.159526\n",
            "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.211332\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.163389\n",
            "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.207089\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.197937\n",
            "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.102269\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.102301\n",
            "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.135866\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.284385\n",
            "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.176004\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.216021\n",
            "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.058585\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.116705\n",
            "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.126740\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.143289\n",
            "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.142736\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.347859\n",
            "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.157658\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.068891\n",
            "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.138493\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.123007\n",
            "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.203444\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.668019\n",
            "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.130507\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.060298\n",
            "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.107771\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.179086\n",
            "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.113804\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.058415\n",
            "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.218324\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.088549\n",
            "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.197654\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.138251\n",
            "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.224921\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.186025\n",
            "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.100877\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.202668\n",
            "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.117462\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.110794\n",
            "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.092282\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.212919\n",
            "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.130221\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.080435\n",
            "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.116270\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.132700\n",
            "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.106094\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.105880\n",
            "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.146057\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.091583\n",
            "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.099481\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.048219\n",
            "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.236348\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.079385\n",
            "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.155455\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.242489\n",
            "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.239663\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.097990\n",
            "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.070440\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.235403\n",
            "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.396565\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.439818\n",
            "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.187920\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.146344\n",
            "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.289022\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.133960\n",
            "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.136036\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.152565\n",
            "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.086214\n",
            "\n",
            "Test set: Avg. loss: 0.0528, Accuracy: 9825/10000 (98%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.053262\n",
            "Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.099520\n",
            "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.084722\n",
            "Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.120063\n",
            "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.247592\n",
            "Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.100792\n",
            "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.139800\n",
            "Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.245559\n",
            "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.197575\n",
            "Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.252651\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.049086\n",
            "Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.287297\n",
            "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.190781\n",
            "Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.233889\n",
            "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.214820\n",
            "Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.098405\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.127840\n",
            "Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.118148\n",
            "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.287194\n",
            "Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.307131\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.370396\n",
            "Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.060741\n",
            "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.192662\n",
            "Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.218283\n",
            "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.171802\n",
            "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.218421\n",
            "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.066008\n",
            "Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.105409\n",
            "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.291268\n",
            "Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.182948\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.195391\n",
            "Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.095840\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.336967\n",
            "Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.086704\n",
            "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.143261\n",
            "Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.220805\n",
            "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.123274\n",
            "Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.049481\n",
            "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.030708\n",
            "Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.228417\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.130120\n",
            "Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.131375\n",
            "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.167351\n",
            "Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.247353\n",
            "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.290014\n",
            "Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.142453\n",
            "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.264589\n",
            "Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.292002\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.136157\n",
            "Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.203786\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.234175\n",
            "Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.273588\n",
            "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.293245\n",
            "Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.059823\n",
            "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.072869\n",
            "Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.157005\n",
            "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.143863\n",
            "Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.350073\n",
            "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.196735\n",
            "Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.235815\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.170235\n",
            "Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.327715\n",
            "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.270264\n",
            "Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.058366\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.149928\n",
            "Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.121734\n",
            "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.090380\n",
            "Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.124217\n",
            "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.127985\n",
            "Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.113150\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.077484\n",
            "Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.225455\n",
            "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.313347\n",
            "Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.188138\n",
            "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.226277\n",
            "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.168680\n",
            "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.206666\n",
            "Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.078418\n",
            "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.116681\n",
            "Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.151683\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.017490\n",
            "Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.106826\n",
            "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.110365\n",
            "Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.124799\n",
            "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.080463\n",
            "Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.117164\n",
            "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.131559\n",
            "Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.249048\n",
            "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.228739\n",
            "Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.061897\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.142259\n",
            "Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.066408\n",
            "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.063594\n",
            "Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.190105\n",
            "\n",
            "Test set: Avg. loss: 0.0502, Accuracy: 9844/10000 (98%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.217140\n",
            "Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.081012\n",
            "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.115391\n",
            "Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.273773\n",
            "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.213241\n",
            "Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.072205\n",
            "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.123237\n",
            "Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.150367\n",
            "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.115350\n",
            "Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.117247\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.065146\n",
            "Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.070911\n",
            "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.075120\n",
            "Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.191362\n",
            "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.053358\n",
            "Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.188387\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.202382\n",
            "Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.079713\n",
            "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.232076\n",
            "Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.143909\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.124302\n",
            "Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.032316\n",
            "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.206211\n",
            "Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.165405\n",
            "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.232650\n",
            "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.225150\n",
            "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.050535\n",
            "Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.101705\n",
            "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.303536\n",
            "Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.185778\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.129742\n",
            "Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.261235\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.119452\n",
            "Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.486412\n",
            "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.122494\n",
            "Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.099453\n",
            "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.198110\n",
            "Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.203528\n",
            "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.522205\n",
            "Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.167975\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.149711\n",
            "Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.186872\n",
            "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.103197\n",
            "Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.279909\n",
            "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.076193\n",
            "Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.134089\n",
            "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.137424\n",
            "Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.061472\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.273827\n",
            "Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.203712\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.210731\n",
            "Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.114438\n",
            "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.142430\n",
            "Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.166457\n",
            "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.177818\n",
            "Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.287949\n",
            "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.127295\n",
            "Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.157457\n",
            "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.209117\n",
            "Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.122275\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.089845\n",
            "Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.156756\n",
            "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.121863\n",
            "Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.248643\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.322309\n",
            "Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.174716\n",
            "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.084028\n",
            "Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.058208\n",
            "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.123928\n",
            "Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.168207\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.119956\n",
            "Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.127470\n",
            "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.144111\n",
            "Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.129774\n",
            "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.107568\n",
            "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.191652\n",
            "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.256260\n",
            "Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.141253\n",
            "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.136788\n",
            "Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.090567\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.101871\n",
            "Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.102709\n",
            "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.147611\n",
            "Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.064568\n",
            "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.045937\n",
            "Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.087749\n",
            "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.064574\n",
            "Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.184517\n",
            "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.072063\n",
            "Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.114307\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.127886\n",
            "Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.183383\n",
            "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.159121\n",
            "Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.193640\n",
            "\n",
            "Test set: Avg. loss: 0.0476, Accuracy: 9842/10000 (98%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.060272\n",
            "Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.089310\n",
            "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.168284\n",
            "Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.296257\n",
            "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.242739\n",
            "Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.143438\n",
            "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.143948\n",
            "Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.184716\n",
            "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.121853\n",
            "Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.311822\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.113135\n",
            "Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.072440\n",
            "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.070701\n",
            "Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.131336\n",
            "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.183687\n",
            "Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.062047\n",
            "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.139546\n",
            "Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.420076\n",
            "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.264225\n",
            "Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.015322\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.220710\n",
            "Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.124105\n",
            "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.147279\n",
            "Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.093402\n",
            "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.169333\n",
            "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.166913\n",
            "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.128169\n",
            "Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.126351\n",
            "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.266713\n",
            "Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.148795\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.124006\n",
            "Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.188548\n",
            "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.116472\n",
            "Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.186430\n",
            "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.082504\n",
            "Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.171737\n",
            "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.059733\n",
            "Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.094527\n",
            "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.222539\n",
            "Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.101733\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.207985\n",
            "Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.099301\n",
            "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.106700\n",
            "Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.207438\n",
            "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.075072\n",
            "Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.253081\n",
            "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.055369\n",
            "Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.086786\n",
            "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.194055\n",
            "Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.094999\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.052713\n",
            "Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.116860\n",
            "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.124941\n",
            "Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.048801\n",
            "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.292455\n",
            "Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.120047\n",
            "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.108022\n",
            "Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.157406\n",
            "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.074071\n",
            "Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.203369\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.116597\n",
            "Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.114778\n",
            "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.149087\n",
            "Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.122026\n",
            "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.057686\n",
            "Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.098862\n",
            "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.122973\n",
            "Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.116072\n",
            "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.049111\n",
            "Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.276766\n",
            "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.329049\n",
            "Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.185672\n",
            "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.031571\n",
            "Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.113415\n",
            "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.132624\n",
            "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.066293\n",
            "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.135751\n",
            "Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.158871\n",
            "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.187074\n",
            "Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.059488\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.084489\n",
            "Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.219573\n",
            "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.114093\n",
            "Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.081127\n",
            "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.254429\n",
            "Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.224670\n",
            "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.060225\n",
            "Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.128076\n",
            "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.027454\n",
            "Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.171211\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.158526\n",
            "Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.396012\n",
            "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.214980\n",
            "Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.135141\n",
            "\n",
            "Test set: Avg. loss: 0.0448, Accuracy: 9863/10000 (99%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.121625\n",
            "Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.035858\n",
            "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.159196\n",
            "Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.074380\n",
            "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.166543\n",
            "Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.039240\n",
            "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.068910\n",
            "Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.252897\n",
            "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.206007\n",
            "Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.112601\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.136673\n",
            "Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.063906\n",
            "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.078977\n",
            "Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.208332\n",
            "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.071145\n",
            "Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.292386\n",
            "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.124247\n",
            "Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.050151\n",
            "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.178622\n",
            "Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.127373\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.050944\n",
            "Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.173464\n",
            "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.205586\n",
            "Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.031421\n",
            "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.099147\n",
            "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.233804\n",
            "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.140131\n",
            "Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.064888\n",
            "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.252062\n",
            "Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.209907\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.528189\n",
            "Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.092554\n",
            "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.196287\n",
            "Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.141049\n",
            "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.110990\n",
            "Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.072067\n",
            "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.092039\n",
            "Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.079583\n",
            "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.130706\n",
            "Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.219433\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.089522\n",
            "Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.190027\n",
            "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.142236\n",
            "Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.119341\n",
            "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.149520\n",
            "Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.112912\n",
            "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.188023\n",
            "Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.170618\n",
            "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.070990\n",
            "Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.128239\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.051928\n",
            "Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.035049\n",
            "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.117888\n",
            "Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.180209\n",
            "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.046377\n",
            "Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.090348\n",
            "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.184975\n",
            "Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.060816\n",
            "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.122828\n",
            "Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.095926\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.247220\n",
            "Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.174073\n",
            "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.385499\n",
            "Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.160290\n",
            "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.157842\n",
            "Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.117252\n",
            "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.118230\n",
            "Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.104055\n",
            "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.211931\n",
            "Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.090136\n",
            "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.111458\n",
            "Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.195658\n",
            "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.079020\n",
            "Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.159624\n",
            "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.109223\n",
            "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.080833\n",
            "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.137077\n",
            "Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.269123\n",
            "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.128299\n",
            "Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.040766\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.021753\n",
            "Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.140398\n",
            "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.070891\n",
            "Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.084090\n",
            "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.155474\n",
            "Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.182905\n",
            "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.073087\n",
            "Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.269602\n",
            "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.124845\n",
            "Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.060147\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.263955\n",
            "Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.087865\n",
            "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.229425\n",
            "Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.138240\n",
            "\n",
            "Test set: Avg. loss: 0.0471, Accuracy: 9858/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#评估模型的性能"
      ],
      "metadata": {
        "id": "zRl2Luu6tLVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "就是这样。仅仅经过3个阶段的训练，我们已经能够达到测试集97%的准确率!我们开始使用随机初始化的参数，正如预期的那样，在开始训练之前，测试集的准确率只有10%左右。\n",
        "\n",
        "我们来画一下训练曲线。"
      ],
      "metadata": {
        "id": "A_vXgiTBtO0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "plt.plot(train_counter, train_losses, color='blue')\n",
        "plt.scatter(test_counter, test_losses, color='red')\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('number of training examples seen')\n",
        "plt.ylabel('negative log likelihood loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "z6RI7_OYtOgH",
        "outputId": "700f7873-0d81-44b5-9099-f79b7b5781e6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-5d8b32dcb187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test Loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'number of training examples seen'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2809\u001b[0m         \u001b[0mverts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deprecated_parameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2810\u001b[0m         edgecolors=None, *, plotnonfinite=False, data=None, **kwargs):\n\u001b[0;32m-> 2811\u001b[0;31m     __ret = gca().scatter(\n\u001b[0m\u001b[1;32m   2812\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4389\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4391\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "从训练曲线来看，看起来我们甚至可以继续训练几个epoch!\n",
        "\n",
        "但在此之前，让我们再看看几个例子，正如我们之前所做的，并比较模型的输出。"
      ],
      "metadata": {
        "id": "-bFS8ROXKjch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "with torch.no_grad():\n",
        "  output = network(example_data)\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Prediction: {}\".format(\n",
        "    output.data.max(1, keepdim=True)[1][i].item()))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eip_T_aFtKvK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "d741b541-2ac5-4bbb-b68f-f85251b87cf5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-f14b5dfe1d84>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#检查点的持续训练"
      ],
      "metadata": {
        "id": "BJRiiuw0K5TD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在让我们继续对网络进行训练，或者看看如何从第一次培训运行时保存的state_dicts中继续进行训练。我们将初始化一组新的网络和优化器。"
      ],
      "metadata": {
        "id": "QCLh-3U2K8TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "continued_network = Net()\n",
        "continued_optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
        "                                momentum=momentum)"
      ],
      "metadata": {
        "id": "qIj8IgqyK3v7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用.load_state_dict()，我们现在可以加载网络的内部状态，并在最后一次保存它们时优化它们。"
      ],
      "metadata": {
        "id": "c5FjHHRjK_sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network_state_dict = torch.load('model.pth')\n",
        "continued_network.load_state_dict(network_state_dict)\n",
        "optimizer_state_dict = torch.load('optimizer.pth')\n",
        "continued_optimizer.load_state_dict(optimizer_state_dict)"
      ],
      "metadata": {
        "id": "XotaPer-LAvG"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "同样，运行一个训练循环应该立即恢复我们之前的训练。为了检查这一点，我们只需使用与前面相同的列表来跟踪损失值。由于我们为所看到的训练示例的数量构建测试计数器的方式，我们必须在这里手动添加它。"
      ],
      "metadata": {
        "id": "CtAKgvY1Ltbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4, 10):\n",
        "    test_counter.append(i*len(train_loader.dataset))\n",
        "    train(i)\n",
        "    test()"
      ],
      "metadata": {
        "id": "edOUQvWJLusN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "72843a99-4459-43bf-d766-e37cb8aebcfe"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-f14b5dfe1d84>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.051055\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.147531\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.131629\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.173572\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.136657\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.151259\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.209322\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.249608\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.041972\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.145205\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.228975\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.113675\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.056486\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.047806\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-ec2ea8a528d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtest_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-702dfd84d115>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "太棒了!我们再次看到测试集的准确性从一个epoch到另一个epoch有了(运行更慢的，慢的多了)提高。让我们用图像来进一步检查训练进度。"
      ],
      "metadata": {
        "id": "tr6Hq1sIL2w1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(train_counter, train_losses, color='blue')\n",
        "plt.scatter(test_counter, test_losses, color='red')\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('number of training examples seen')\n",
        "plt.ylabel('negative log likelihood loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ye8MVBRaL0ZF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "44ebf72a-476d-4e7b-a8e9-71c9d7b6fa99"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}